defaultPodOptions:
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          preference:
            matchExpressions:
              - key: kubernetes.io/hostname
                operator: In
                values:
                  - hyper-ryzen

controllers:
  garm:
    strategy: Recreate
    serviceAccount:
      identifier: garm
    initContainers:
      init-garm:
        envFrom:
          - secretRef:
              name: garm-secrets
        image:
          repository: alpine
          pullPolicy: IfNotPresent
          tag: 3.23
        command:
          - /scripts/init-garm.sh
    containers:
      garm:
        image:
          repository: ghcr.io/cloudbase/garm
          pullPolicy: IfNotPresent
          tag: v0.1.6
        resources:
          requests:
            cpu: 5m
            memory: 256Mi
          limits:
            memory: 256Mi
  update-pools:
    type: job
    annotations:
      # ArgoCD hook annotations
      "argocd.argoproj.io/hook": PostSync
      "argocd.argoproj.io/hook-delete-policy": BeforeHookCreation
      "argocd.argoproj.io/sync-wave": "9999"
      # Helm hook annotations
      "helm.sh/hook": post-install,post-upgrade
      "helm.sh/hook-delete-policy": before-hook-creation
      "helm.sh/hook-weight": "9999"
    containers:
      main:
        image:
          repository: alpine/curl
          pullPolicy: IfNotPresent
          tag: 8.17.0
        env:
          # renovate: datasource=docker depName=garm-k8s-runner packageName=ghcr.io/ionutbalutoiu/garm-k8s-runner
          GARM_K8S_RUNNER_IMAGE_TAG: 2.329.0-ce65795
        envFrom:
          - secretRef:
              name: garm-secrets
        command:
          - /scripts/update-garm-pools.sh
        resources:
          requests:
            cpu: 5m
            memory: 128Mi
          limits:
            memory: 128Mi

service:
  garm:
    controller: garm
    ports:
      http:
        port: 80

serviceAccount:
  garm:
    enabled: true

rbac:
  roles:
    role:
      type: Role
      rules:
        - apiGroups:
            - "*"
          resources:
            - "*"
          verbs:
            - "*"
  bindings:
    role-binding:
      type: RoleBinding
      roleRef:
        identifier: role
      subjects:
        - identifier: garm

configMaps:
  scripts:
    data:
      update-garm-pools.sh: |
        #!/bin/sh
        set -eo pipefail

        if [[ -z "${GARM_K8S_RUNNER_IMAGE_TAG}" ]]; then
          echo "GARM_K8S_RUNNER_IMAGE_TAG is not set. Exiting."
          exit 1
        fi

        apk add -q --no-cache jq

        for POOL_ID in $(curl -sS \
                          -H "Accept: application/json" \
                          -H "Authorization: Bearer $CLI_BEARER_TOKEN" \
                          "https://garm.internal.balutoiu.com/api/v1/pools" | \
                            jq -r '.[] | select(.image | startswith("ghcr.io/ionutbalutoiu/garm-k8s-runner")) | .id'); do
          echo "Updating K8s pool ${POOL_ID} to use image: ghcr.io/ionutbalutoiu/garm-k8s-runner:${GARM_K8S_RUNNER_IMAGE_TAG}"
          curl -sS \
            -X PUT \
            -H "Accept: application/json" \
            -H "Content-Type: application/json" \
            -H "Authorization: Bearer $CLI_BEARER_TOKEN" \
              "https://garm.internal.balutoiu.com/api/v1/pools/${POOL_ID}" \
              -d "{\"image\":\"ghcr.io/ionutbalutoiu/garm-k8s-runner:${GARM_K8S_RUNNER_IMAGE_TAG}\"}"
        done

      init-garm.sh: |
        #!/bin/sh
        set -eo pipefail

        CONFIG_TEMPLATE="/config-templates/config-template.toml"
        CONFIG_PATH="/etc/garm/config.toml"

        apk --quiet add envsubst

        echo "Generating GARM config at ${CONFIG_PATH} from template ${CONFIG_TEMPLATE}"
        touch ${CONFIG_PATH}
        chmod 600 ${CONFIG_PATH}
        cat ${CONFIG_TEMPLATE} | envsubst > ${CONFIG_PATH}

  config-templates:
    data:
      config-template.toml: |
        [default]
        enable_webhook_management = true

        [logging]
        enable_log_streamer = true
        log_format = "text"
        log_level = "info"
        log_source = false

        [metrics]
        enable = true
        disable_auth = false

        [jwt_auth]
        secret = "${JWT}"
        time_to_live = "8760h"

        [apiserver]
          bind = "0.0.0.0"
          port = 80
          use_tls = false

        [database]
          backend = "sqlite3"
          passphrase = "${DB_PASSPHRASE}"
          [database.sqlite3]
            db_file = "/etc/garm/garm.db"

        [[provider]]
          name = "lxd_hyper_ryzen"
          provider_type = "external"
          description = "Hyper-Ryzen LXD installation"
          [provider.external]
            provider_executable = "/opt/garm/providers.d/garm-provider-lxd"
            config_file = "/garm-provider-configs/lxd.toml"

        [[provider]]
          name = "k8s_home"
          description = "Home K3s"
          provider_type = "external"
          [provider.external]
            provider_executable = "/opt/garm/providers.d/garm-provider-k8s"
            config_file = "/garm-provider-configs/k8s.yaml"
            environment_variables = ["KUBERNETES_"]

  provider-configs:
    data:
      lxd.toml: |
        # When defining a pool for a repository or an organization, you have an option to
        # specify a "flavor". In LXD terms, this translates to "profiles". Profiles allow
        # you to customize your instances (memory, cpu, disks, nics, etc).
        # This option allows you to inject the "default" profile along with the profile selected
        # by the flavor.
        include_default_profile = false

        # instance_type defines the type of instances this provider will create.
        #
        # Options are:
        #
        #   * virtual-machine (default)
        #   * container
        #
        instance_type = "virtual-machine"

        # enable/disable secure boot. If the image you select for the pool does not have a
        # signed bootloader, set this to false, otherwise your instances won't boot.
        secure_boot = false

        # Project name to use. You can create a separate project in LXD for runners.
        project_name = "garm"

        # URL is the address on which LXD listens for connections (ex: https://example.com:8443)
        url = "https://10.119.10.100:8447"

        # garm supports certificate authentication for LXD remote connections. The easiest way
        # to get the needed certificates, is to install the lxc client and add a remote. The
        # client_certificate, client_key and tls_server_certificate can be then fetched from
        # $HOME/snap/lxd/common/config.
        client_certificate = "/lxd-certs/client.crt"
        client_key = "/lxd-certs/client.key"
        tls_server_certificate = "/lxd-certs/server.crt"

        [image_remotes]
            # Image remotes are important. These are the default remotes used by lxc. The names
            # of these remotes are important. When specifying an "image" for the pool, that image
            # can be a hash of an existing image on your local LXD installation or it can be a
            # remote image from one of these remotes. You can specify the images as follows:
            # Example:
            #
            #    * ubuntu:20.04
            #    * ubuntu_daily:20.04
            #    * images:centos/8/cloud
            #
            # Ubuntu images come pre-installed with cloud-init which we use to set up the runner
            # automatically and customize the runner. For non Ubuntu images, you need to use the
            # variant that has "/cloud" in the name. Those images come with cloud-init.

            [image_remotes.ubuntu]
            addr = "https://cloud-images.ubuntu.com/releases"
            public = true
            protocol = "simplestreams"
            skip_verify = false

            [image_remotes.ubuntu_daily]
            addr = "https://cloud-images.ubuntu.com/daily"
            public = true
            protocol = "simplestreams"
            skip_verify = false

            [image_remotes.images]
            addr = "https://images.linuxcontainers.org"
            public = true
            protocol = "simplestreams"
            skip_verify = false

      k8s.yaml: |
        runnerNamespace: garm
        flavors:
          garm:
            requests:
              cpu: 25m
              memory: 512Mi
            limits:
              memory: 2Gi

route:
  garm:
    parentRefs:
      - name: traefik-gateway
        namespace: traefik-internal
    hostnames:
      - garm.internal.balutoiu.com
    rules:
      - name: api
        matches:
          - path:
              type: PathPrefix
              value: /
        backendRefs:
          - identifier: garm
  webhooks:
    parentRefs:
      - name: traefik-gateway
        namespace: traefik-external
    hostnames:
      - garm.balutoiu.com
    rules:
      - name: webhooks
        matches:
          - path:
              type: PathPrefix
              value: /webhooks
        backendRefs:
          - identifier: garm

persistence:
  scripts:
    type: configMap
    identifier: scripts
    defaultMode: 0755
    advancedMounts:
      garm:
        init-garm:
          - path: /scripts
      update-pools:
        main:
          - path: /scripts

  config-templates:
    type: configMap
    identifier: config-templates
    advancedMounts:
      garm:
        init-garm:
          - path: /config-templates

  lxd-certs:
    type: secret
    name: garm-lxd-certs
    defaultMode: 0600
    advancedMounts:
      garm:
        garm:
          - path: /lxd-certs

  provider-configs:
    type: configMap
    identifier: provider-configs
    advancedMounts:
      garm:
        garm:
          - path: /garm-provider-configs

  garm-etc:
    type: persistentVolumeClaim
    storageClass: longhorn-2r
    accessMode: ReadWriteOnce
    size: 3Gi
    advancedMounts:
      garm:
        init-garm:
          - path: /etc/garm
        garm:
          - path: /etc/garm
